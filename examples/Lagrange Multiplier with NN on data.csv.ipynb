{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import cma\n",
    "from scipy.optimize import minimize\n",
    "from scipy import stats\n",
    "\n",
    "STATES = 18\n",
    "ACTIONS = 4\n",
    "GAMMA = 0.95\n",
    "BENCHMARK_PERFORMANCE = 1.41537\n",
    "DUMMY = 0\n",
    "\n",
    "def parse_file(filename):\n",
    "    fileForInput = open(filename, 'r')\n",
    "    no_of_episodes = int(fileForInput.readline())\n",
    "    no_of_episodes = 1000\n",
    "    print(int(no_of_episodes))\n",
    "    states = [[] for _ in range(no_of_episodes)]\n",
    "    actions = [[] for _ in range(no_of_episodes)]\n",
    "    rewards = [[] for _ in range(no_of_episodes)]\n",
    "    pi_b = [[] for _ in range(no_of_episodes)]\n",
    "\n",
    "    horizon_length = 0\n",
    "    min_timesteps = 1000\n",
    "    for episode in range(no_of_episodes):\n",
    "        timesteps = int(fileForInput.readline())\n",
    "        horizon_length = max(horizon_length, timesteps)\n",
    "        min_timesteps = min(min_timesteps, timesteps)\n",
    "        for timestep in range(timesteps):\n",
    "            cur_list = fileForInput.readline().rstrip('\\n').split(',')\n",
    "            states[episode].append([int(cur_list[0]) + 1])\n",
    "            actions[episode].append(int(cur_list[1]))\n",
    "            rewards[episode].append(int(cur_list[2]))\n",
    "            pi_b[episode].append(float(cur_list[3]))\n",
    "    \n",
    "    # padding the data with dummy values in order to make all episodes of same length\n",
    "    for episode in range(no_of_episodes):\n",
    "        cur_timesteps = len(states[episode])\n",
    "        for timestep in range(cur_timesteps, horizon_length):\n",
    "            states[episode].append([DUMMY])\n",
    "            actions[episode].append(DUMMY)\n",
    "            rewards[episode].append(DUMMY)\n",
    "            pi_b[episode].append(DUMMY)\n",
    "    dataset = {'states': states, 'actions': actions, 'rewards': rewards, 'pi_b': pi_b}\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed forward neural network for mapping states to actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=1, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, states, actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(states, 128)  \n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64,32)\n",
    "        self.fc4 = nn.Linear(32, actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        episodes = x.size()[0]\n",
    "        horizon_length = x.size()[1]\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        out = self.fc4(x)\n",
    "        out = torch.nn.functional.softmax(out, dim=1)\n",
    "\n",
    "#         out = torch.reshape(out, (episodes, horizon_length, -1))\n",
    "        out = out.view(episodes, horizon_length, -1)\n",
    "        return out\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[2:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net(1, 4)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reading data from file and splitting into test and train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    dataset = parse_file('data.csv')\n",
    "    states = dataset['states']\n",
    "    actions = dataset['actions']\n",
    "    rewards = dataset['rewards']\n",
    "    pi_b = dataset['pi_b']\n",
    "    states_train, states_test, actions_train, actions_test, rewards_train, rewards_test, pi_b_train, pi_b_test = \\\n",
    "        train_test_split(states, actions, rewards, pi_b, test_size=0.5, random_state=42)\n",
    "    \n",
    "    dataset_test = {'states': states_test, 'actions': actions_test, 'rewards': rewards_test, 'pi_b': pi_b_test}\n",
    "    dataset_test_size = len(states_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Converting states, rewards, actions and pi_b into tensors to work with autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensor_rewards = torch.tensor(rewards_train)\n",
    "tensor_states = torch.tensor(states_train)\n",
    "tensor_pib = torch.tensor(pi_b_train)\n",
    "tensor_actions = torch.tensor(actions_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to calculate the high confidence lower bound with ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lower_bound(is_estimates, factor=2, delta=0.01):\n",
    "    size = is_estimates.size()[0]\n",
    "    lb = torch.mean(is_estimates) - factor * (\n",
    "            torch.std(is_estimates) / np.sqrt(size)) * stats.t.ppf(1 - delta, size - 1)\n",
    "    return lb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting data into batches using dataloader and randomsampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, RandomSampler\n",
    "bs = 64\n",
    "dataset = TensorDataset(tensor_states, tensor_actions, tensor_rewards, tensor_pib)\n",
    "sampler = RandomSampler(dataset)\n",
    "dataloader = DataLoader(dataset, sampler=sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function that returns the PDIS estimates of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_PDIS_estimate(dataset, out):\n",
    "    states = dataset['states']\n",
    "    actions = dataset['actions']\n",
    "    rewards = dataset['rewards']\n",
    "    pi_b = dataset['pi_b']\n",
    "\n",
    "    is_estimates = torch.zeros(states.size()[0])\n",
    "\n",
    "    for episode in range(len(states)):\n",
    "        is_current = torch.tensor(0).float()\n",
    "        frac = torch.tensor(1).float()\n",
    "        for timestep in range(len(states[episode])):\n",
    "            \n",
    "            s = states[episode][timestep]\n",
    "            if s[0] == DUMMY:\n",
    "                break\n",
    "            a = actions[episode][timestep]\n",
    "            frac *= out[episode][timestep][a] / pi_b[episode][timestep]\n",
    "            is_current += (GAMMA ** timestep) * (rewards[episode][timestep] * frac)\n",
    "        is_estimates[episode] =is_current\n",
    "    return is_estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of the neural network with loss as \n",
    "$$-PDIS (\\theta) + \\lambda * (c - \\rho^-)$$\n",
    "Minimization w.r.t. policy parameter $\\theta $  and \n",
    "Maximization w.r.t. lagrange multiplier $\\lambda$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch loss-----> -2.1430511474609375\n",
      "epoch loss-----> -2.370282858610153\n",
      "epoch loss-----> -2.690591514110565\n",
      "epoch loss-----> -3.010955587029457\n",
      "epoch loss-----> -3.2746819853782654\n"
     ]
    }
   ],
   "source": [
    "horizon_length = 186\n",
    "batch_size = 64\n",
    "# using Adam optimizer to solve the minimization w.r.t policy parameter\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = 1e-4)\n",
    "\n",
    "# langrange multiplier\n",
    "l = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "# getting the value of the total objective function at the current values of theta and lambda\n",
    "def lfn(out, l, dataset):\n",
    "    list_estimates = (get_PDIS_estimate(dataset, out))\n",
    "    mean_estimate = -torch.mean(list_estimates) + l * (BENCHMARK_PERFORMANCE - lower_bound(list_estimates))\n",
    "    return mean_estimate\n",
    "\n",
    "# training loop\n",
    "for epoch in range(5):\n",
    "    epoch_loss = 0 \n",
    "    c=0\n",
    "    for states, actions, rewards, pi_b in dataloader:\n",
    "        episodes = states.size()[0]\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass to get the action probabilities from the states\n",
    "        out = net(states.float())\n",
    "        \n",
    "        # calculate the objective function\n",
    "        mean_estimate = lfn(out, l, {'states':states, 'actions': actions,'rewards':rewards,'pi_b':pi_b})\n",
    "        \n",
    "        # backprop w.r.t. policy parameter gradient descent since minimization w.r.t. theta\n",
    "        mean_estimate.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        epoch_loss += mean_estimate.item()\n",
    "        c+=1\n",
    "        \n",
    "        # calculating the gradient w.r.t. lambda\n",
    "        _, dl = torch.autograd.grad(mean_estimate, [out, l])\n",
    "        \n",
    "        # gradient ascent w.r.t. lambda since maximization w.r.t. lambda\n",
    "        with torch.no_grad():\n",
    "            l += 1e-5 * dl\n",
    "#         print(\"l is \", l)\n",
    "    print(\"epoch loss----->\",epoch_loss/c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analyzing the learned model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "test_states = torch.tensor([[1],[2],[3],[4],[5],[6],[7],[8],[9],[10],[11],[12],[13],[14],[15],[16],[17]])\n",
    "pi_e = net(test_states.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2408, 0.2459, 0.2227, 0.2907]],\n",
      "\n",
      "        [[0.2331, 0.2475, 0.2220, 0.2975]],\n",
      "\n",
      "        [[0.2253, 0.2472, 0.2196, 0.3079]],\n",
      "\n",
      "        [[0.2174, 0.2451, 0.2171, 0.3205]],\n",
      "\n",
      "        [[0.2075, 0.2439, 0.2136, 0.3350]],\n",
      "\n",
      "        [[0.1979, 0.2424, 0.2093, 0.3505]],\n",
      "\n",
      "        [[0.1884, 0.2407, 0.2049, 0.3660]],\n",
      "\n",
      "        [[0.1791, 0.2388, 0.2003, 0.3817]],\n",
      "\n",
      "        [[0.1701, 0.2367, 0.1957, 0.3975]],\n",
      "\n",
      "        [[0.1613, 0.2344, 0.1910, 0.4133]],\n",
      "\n",
      "        [[0.1528, 0.2318, 0.1860, 0.4294]],\n",
      "\n",
      "        [[0.1446, 0.2288, 0.1808, 0.4458]],\n",
      "\n",
      "        [[0.1366, 0.2257, 0.1754, 0.4623]],\n",
      "\n",
      "        [[0.1289, 0.2224, 0.1698, 0.4789]],\n",
      "\n",
      "        [[0.1215, 0.2188, 0.1643, 0.4954]],\n",
      "\n",
      "        [[0.1144, 0.2150, 0.1587, 0.5119]],\n",
      "\n",
      "        [[0.1076, 0.2110, 0.1531, 0.5283]]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(pi_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_rewards = torch.tensor(rewards_test)\n",
    "tensor_states = torch.tensor(states_test)\n",
    "tensor_pib = torch.tensor(pi_b_test)\n",
    "tensor_actions = torch.tensor(actions_test)\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, RandomSampler\n",
    "bs = 64\n",
    "dataset = TensorDataset(tensor_states, tensor_actions, tensor_rewards, tensor_pib)\n",
    "# train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "sampler = RandomSampler(dataset)\n",
    "dataloader = DataLoader(dataset, sampler=sampler, batch_size=bs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the learned model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.2031)\n",
      "tensor(-3.5412)\n",
      "tensor(-1.7411)\n",
      "tensor(-1.4903)\n",
      "tensor(-2.5011)\n",
      "tensor(-2.2867)\n",
      "tensor(-2.3453)\n",
      "tensor(-2.9590)\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "net.eval()\n",
    "for states, actions, rewards, pi_b in dataloader:\n",
    "    with torch.no_grad():\n",
    "        out = net(states.float())\n",
    "        mean_estimate = lfn(out, l, {'states':states, 'actions': actions,'rewards':rewards,'pi_b':pi_b})\n",
    "        print(mean_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lfn = lambda x, l: x**2 / 2 + l * (x - 1)\n",
    "# x = torch.tensor(-0.1, requires_grad=True)\n",
    "# l = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "# xs = []\n",
    "# ls = []\n",
    "# for i in range(200):\n",
    "#     f = lfn(x, l)\n",
    "#     dx, dl = torch.autograd.grad(f, [x, l])\n",
    "#     with torch.no_grad():\n",
    "#         x -= 1e-1 * dx\n",
    "#         l += 1e-1 * dl\n",
    "#     xs.append(x.item())\n",
    "#     ls.append(l.item())\n",
    "\n",
    "# %matplotlib inline\n",
    "# from matplotlib import pyplot\n",
    "# # pyplot.plot(xs)\n",
    "# pyplot.plot(ls)\n",
    "# x.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
